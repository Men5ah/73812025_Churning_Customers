# -*- coding: utf-8 -*-
"""Homework.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HPEQ2lRUptHxd10LdwdQ-Cua-lLEkTV0

# Problem Statement
Problem goes here

# Import Libraries
"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow
from tensorflow import keras

"""# Import Dataset"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Customer-Churning/CustomerChurn_dataset.csv")

"""# Exploratory Data Analysis"""

df.drop(['customerID'], axis=1, inplace=True)
df['SeniorCitizen'] = df['SeniorCitizen'].astype('object')
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

print("The shape of the dataset is: ", df.shape)

df.head()

df.info()

"""Findings:
1. There are no null values so there is no need for imputation.
2. Churn is the target value.
3. There are 18 non-numerical attributes and 3 numerical attributes.
"""

df.describe(include='all').T

df.isnull().sum()

"""## Exploring type Object"""

# objects = df.select_dtypes(exclude=["number"])
objects = [i for i in df.columns if df[i].dtype=='O']
print("There are", len(objects), "object attributes")
print("These are:", objects)

df[objects].head()

for i in objects:
    print(df[i].value_counts())

"""## Exploring Target Attribute"""

# Check for any null values
df['Churn'].isnull().sum()

# Number of distinct values
df['Churn'].nunique()

#View the unique values in Churn
df['Churn'].unique()

# Check how many of each unique value there are
df['Churn'].value_counts()

# Check how many of each unique value there are
df['Churn'].value_counts()/len(df)

ax = plt.subplots(figsize=(18,8))

ax = df['Churn'].value_counts().plot.pie(autopct='%1.1f%%')
ax.set_title("Churn Share")

# List of attributes excluding the target 'Churn'
attributes = df.columns[df.columns != 'Churn']

# Plotting the relationship between each attribute and 'Churn'
for attribute in attributes:
    if df[attribute].dtype == 'object':
        # Categorical attributes
        plt.figure(figsize=(10, 6))
        sns.countplot(x=attribute, hue='Churn', data=df)
        plt.title(f'Relationship between {attribute} and Churn')
        plt.show()
    else:
        # Numerical attributes
        plt.figure(figsize=(10, 6))
        sns.kdeplot(data=df, x=attribute, hue='Churn', fill=True)
        plt.title(f'Relationship between {attribute} and Churn')
        plt.show()

"""## Exploring type Number"""

numbers = [i for i in df.columns if df[i].dtype!='O']
print("There are", len(numbers), "numerical attributes")
print("These are:", numbers)

df[numbers].head()

df[numbers].isnull().sum()

imputer = SimpleImputer(strategy='mean')

imputer.fit(df['TotalCharges'].values.reshape(-1, 1))
transformed = imputer.transform(df['TotalCharges'].values.reshape(-1, 1))
df['TotalCharges'] = transformed

df[numbers].isnull().sum()

for i in numbers:
    print(i,":",df[i].nunique(), "unique values")

f, ax = plt.subplots(figsize=(10, 8))
ax = sns.boxplot(x="Churn", y="tenure", data=df)
ax.set_title("")
plt.show()

f, ax = plt.subplots(figsize=(10, 8))
ax = sns.boxplot(x="Churn", y="MonthlyCharges", data=df)
ax.set_title("")
plt.show()

f, ax = plt.subplots(figsize=(10, 8))
ax = sns.boxplot(x="Churn", y="TotalCharges", data=df)
ax.set_title("")
plt.show()

df

"""# Feature Enginering"""

categorical = [col for col in df.columns if df[col].dtypes == 'O']

df[categorical].isnull().mean()

numerical = [col for col in df.columns if df[col].dtypes != 'O']
df[numerical].isnull().mean()

# import category_encoders as ce
#Encode
label = LabelEncoder()

columns = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
                         'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',
                         'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']

for i in columns:
    df[i] = label.fit_transform(df[i])
df
#

"""# Scaling"""

columns_for_scaling = df.columns

scaler = StandardScaler()

numerical_columns = ['tenure','MonthlyCharges','TotalCharges']
df_to_scale = df[numerical]
df_scaled= scaler.fit_transform(df_to_scale)
df_scaled = pd.DataFrame(df_scaled, columns=numerical_columns)

for i in numerical_columns:
    df.drop([i], axis=1, inplace=True)
df = pd.concat([df, df_scaled], axis=1)
df

"""# Feature and Target Selection"""

y = df["Churn"]
X = df.drop(["Churn"], axis=1)

"""# Splitting"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape

"""# Random Forest"""

rfr = RandomForestClassifier(n_estimators=10)
rfr.fit(X_train, y_train)

y_pred = rfr.predict(X_test)

print(accuracy_score(y_test, y_pred))

rfr_100 = RandomForestClassifier(n_estimators=100, random_state= 42)

rfr_100.fit(X_train, y_train)
y_pred_100 = rfr_100.predict(X_test)

print(accuracy_score(y_test, y_pred))

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

feature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)

feature_scores

X_train = X_train.drop(['PhoneService'], axis=1)
X_test = X_test.drop(['PhoneService'], axis=1)

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print(accuracy_score(y_test, y_pred))

"""# Confusion Matrix"""

from sklearn.metrics import confusion_matrix

matrix = confusion_matrix(y_test, y_pred)

print(matrix)

"""# Cross Validation and Grid Search"""

# from sklearn.model_selection import GridSearchCV

# param_grid = {
#      'n_estimators': [10, 50, 100, 150],
#     'max_depth': [None, 10, 20, 30],
#     'min_samples_split': [2, 5, 10, 15],
#     'min_samples_leaf': [1, 2, 4, 8],
# }

# clf = RandomForestClassifier()
# grid_search = GridSearchCV(
#     clf, param_grid, cv=3, n_jobs=-1, verbose=1, scoring='accuracy'
# )

# grid_search.fit(X_train, y_train)

# # Print the best parameters and corresponding accuracy
# print("Best Parameters: ", grid_search.best_params_)
# print("Best Accuracy: ", grid_search.best_score_)

# # Evaluate the best model on the test set
# best_model = grid_search.best_estimator_
# test_accuracy = best_model.score(X_test, y_test)
# print("Test Accuracy: ", test_accuracy)

"""# KERAS MULTI LAYER PERCEPTRON USING THE FUNCTIONAL API"""

#Importing necessary libraries
import keras
from keras.models import Model, Sequential
from keras.layers import Input, Dense
from keras.optimizers import Adam, RMSprop
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping
import tensorflow as tf

# Xtrain, X_test, ytrain, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.2, random_state=42)

input_nodes = Input(shape=(X_train.shape[1],))
hidden1 = Dense(128, activation='relu')(input_nodes)
hidden2 = Dense(64, activation='relu')(hidden1)
hidden3 = Dense(32, activation='relu')(hidden2)
hidden4 = Dense(16, activation='relu')(hidden3)
output_layer = Dense(1, activation='sigmoid')(hidden4)
model = Model(inputs=input_nodes, outputs=output_layer)

stopping = EarlyStopping(monitor='value_loss', patience=10, mode='min', restore_best_weights=True)

model.compile(optimizer=RMSprop(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])

Xtrain = np.array(X_train, dtype=np.float32)
ytrain = np.array(y_train, dtype=np.float32)
model.fit(Xtrain, y_train, epochs=150, batch_size=50, validation_data=(X_val, y_val), callbacks=[stopping])

_, accuracy = model.evaluate(X_train, y_train)
accuracy*100

loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy*100:.4f}')

!pip install scikeras

from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV
def modelCreate(optimizer=RMSprop):
    input_nodes = Input(shape=(X_train.shape[1],))
    hidden1 = Dense(128, activation='relu')(input_nodes)
    hidden2 = Dense(64, activation='relu')(hidden1)
    hidden3 = Dense(32, activation='relu')(hidden2)
    hidden4 = Dense(16, activation='relu')(hidden3)
    output_layer = Dense(1, activation='sigmoid')(hidden4)
    model = Model(inputs=input_nodes, outputs=output_layer)
    model.compile(optimizer=optimizer(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=modelCreate, verbose = 2)
param_grid = {
    'optimizer': ['SGD', 'RMSprop'],
    'epochs': [100, 150],
    'batch_size': [30, 50]
}

grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)
grid_result = grid.fit(X_train, y_train)
print(grid_result.best_score_, grid_result.best_params_)

#Using best params for training
input_nodes = Input(shape=(X_train.shape[1],))
hidden1 = Dense(128, activation='relu')(input_nodes)
hidden2 = Dense(64, activation='relu')(hidden1)
hidden3 = Dense(32, activation='relu')(hidden2)
hidden4 = Dense(16, activation='relu')(hidden3)
output_layer = Dense(1, activation='sigmoid')(hidden4)
model = Model(inputs=input_nodes, outputs=output_layer)

model.compile(optimizer=RMSprop(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])
model.fit(Xtrain, y_train, epochs=100, batch_size=50, validation_data=(X_val, y_val), callbacks=[stopping])

_, accuracy = model.evaluate(X_train, y_train)
print(accuracy*100)
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy*100:.4f}')



best_model = grid.best_estimator_
best_model
X_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, X_pred)
print(f"Accuracy is: {accuracy*100}")

validation_prediction = best_model.predict(X_val)
validation_accuracy = accuracy_score(y_val, validation_prediction)
print(f"Accuracy is: {validation_accuracy*100}")

from sklearn.metrics import roc_auc_score
roc_auc = roc_auc_score(y_val,validation_prediction)
roc_auc

import pickle
filename = 'scaler.pkl'
pickle.dump(scaler, open(filename, 'wb'))

filename = 'model.pkl'
pickle.dump(model, open(filename, 'wb'))